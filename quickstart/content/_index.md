+++
title = 'Home'
draft = false
+++

<div style="display: flex; align-items: center;">
    <img src="/images/nico.jpg" alt="Nicolas Espinosa Dice" style="width: 25%; margin-right: 20px;">
    <div>
        I am a first year PhD student at <a href="https://cis.cornell.edu/">Cornell University</a>, where I am advised by <a href="https://cis.cornell.edu/">Wen Sun</a>. My research focuses on reinforcement and imitation learning.  
        <br>
        <br>
        Prior to Cornell, I received by B.S. from <a href="https://www.hmc.edu/">Harvey Mudd College</a>, where I was advised by <a href="https://www.cs.hmc.edu/~montanez/">George D. Montanez</a> and <a href="https://www.math.hmc.edu/~dk/">Dagan Karp</a>.  I worked with George D. Montanez in the <a href="https://www.cs.hmc.edu/~montanez/amistad.html">AMISTAD Lab</a> and <a href="https://math.hmc.edu/gu/">Weiqing Gu</a> at <a href="https://data-to-decision.com/">Dasion</a>.
    </div>
</div>

Last updated: August 2024
<!-- I have been fortunate to work with the following great collaborators: Kiante Brantley, Yiding Chen, Gokul Swamy, Owen Oertell, Yiyi Zhang, Sanjiban Choudhury.  -->

## <span id="research">Research</span>

[**Efficient Inverse Reinforcement Learning without Compounding Errors**](https://nico-espinosadice.github.io/efficient-IRL/)  
    [**Nicolas Espinosa Dice**](https://nico-espinosadice.github.io/),
    [Gokul Swamy](https://gokul.dev/),
    [Sanjiban Choudhury](https://www.sanjibanchoudhury.com/),
    [Wen Sun](https://wensun.github.io/)  
    *RLC 2024 RLSW, RLBRew*  
    [Project Page](https://nico-espinosadice.github.io/efficient-IRL/) /
    [Paper](https://nico-espinosadice.github.io/efficient-IRL/static/efficient-irl.pdf) /
    [Code](https://github.com/nico-espinosadice/garage-fork/tree/main)  
    There are two seemingly contradictory desiderata for IRL algorithms: (a) preventing the compounding errors that stymie offline approaches like behavioral cloning and (b) avoiding the worst-case exploration complexity of reinforcement learning (RL). Prior work has been able to achieve either (a) or (b) but not both simultaneously. We prove that, under a novel structural condition we term reward-agnostic policy completeness, efficient IRL algorithms do avoid compounding errors, giving us the best of both worlds.

